{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aumento del Tama√±o del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de poder generar nuevos datos para el datasets debemos de observar nuestros archivos csv y procesarlos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archivos .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resumen_evolucion.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PacienteID       Fecha   Hora  PresionSistolica  PresionDiastolica  \\\n",
      "0            1  01/05/2023  06:00             100.0               65.0   \n",
      "1            1  01/05/2023  14:00             105.0               68.0   \n",
      "2            1  01/05/2023  22:00              98.0               60.0   \n",
      "3            1  02/05/2023  06:00             105.0               70.0   \n",
      "4            1  02/05/2023  14:00             110.0               72.0   \n",
      "5            1  02/05/2023  22:00             100.0               68.0   \n",
      "6            1  03/05/2023  06:00             115.0               75.0   \n",
      "7            1  03/05/2023  14:00             118.0               78.0   \n",
      "8            1  03/05/2023  22:00             110.0               70.0   \n",
      "9            1  04/05/2023  06:00             120.0               80.0   \n",
      "10           1  04/05/2023  14:00             125.0               82.0   \n",
      "11           1  04/05/2023  22:00             115.0               78.0   \n",
      "12           1  05/05/2023  06:00             118.0               78.0   \n",
      "13           1  05/05/2023  14:00             122.0               80.0   \n",
      "14           1  05/05/2023  22:00             115.0               75.0   \n",
      "15           2  10/07/2023  06:00             100.0               60.0   \n",
      "16           2  10/07/2023  14:00             105.0               65.0   \n",
      "17          18         2.0    NaN               NaN                NaN   \n",
      "18           2  10/07/2023  22:00              95.0               58.0   \n",
      "19          22         1.0    NaN               NaN                NaN   \n",
      "20           2  11/07/2023  06:00             110.0               70.0   \n",
      "21           2  11/07/2023  14:00             115.0               75.0   \n",
      "22           2  11/07/2023  22:00             105.0               68.0   \n",
      "23           2  12/07/2023  06:00             115.0               75.0   \n",
      "24           2  12/07/2023  14:00             118.0               78.0   \n",
      "25           2  12/07/2023  22:00             110.0               70.0   \n",
      "26           2  13/07/2023  06:00             120.0               80.0   \n",
      "27           2  13/07/2023  14:00             125.0               85.0   \n",
      "28           2  13/07/2023  22:00             115.0               78.0   \n",
      "\n",
      "    FrecuenciaCardiaca  Temperatura  FrecuenciaRespiratoria  \\\n",
      "0                105.0         38.8                    28.0   \n",
      "1                115.0         39.2                    30.0   \n",
      "2                110.0         39.0                    26.0   \n",
      "3                 95.0         38.0                    24.0   \n",
      "4                100.0         38.5                    26.0   \n",
      "5                 90.0         38.2                    22.0   \n",
      "6                 85.0         37.2                    20.0   \n",
      "7                 90.0         37.5                    22.0   \n",
      "8                 80.0         37.0                    18.0   \n",
      "9                 80.0         36.8                    18.0   \n",
      "10                85.0         37.0                    20.0   \n",
      "11                78.0         36.6                    16.0   \n",
      "12                78.0         36.5                    16.0   \n",
      "13                82.0         36.7                    18.0   \n",
      "14                75.0         36.4                    14.0   \n",
      "15                95.0         37.2                    20.0   \n",
      "16               100.0         37.2                    18.0   \n",
      "17                 NaN          NaN                     NaN   \n",
      "18                90.0         37.5                    16.0   \n",
      "19                 NaN          NaN                     NaN   \n",
      "20                80.0         37.8                    18.0   \n",
      "21                85.0         38.0                    20.0   \n",
      "22                78.0         37.5                    16.0   \n",
      "23                72.0         37.0                    16.0   \n",
      "24                78.0         37.2                    18.0   \n",
      "25                70.0         36.8                    14.0   \n",
      "26                70.0         36.8                    16.0   \n",
      "27                75.0         37.0                    18.0   \n",
      "28                65.0         36.5                    14.0   \n",
      "\n",
      "    SaturacionOxigeno  Glucosa  ...  Creatinina   Urea    AST   ALT  \\\n",
      "0                89.0    110.0  ...         1.1   50.0   30.0  28.0   \n",
      "1                95.0    125.0  ...       134.0    NaN   98.0   NaN   \n",
      "2                94.0    115.0  ...       133.0    NaN   50.0   NaN   \n",
      "3                94.0    100.0  ...       100.0    1.0   40.0  32.0   \n",
      "4                96.0    110.0  ...       136.0    NaN  100.0   NaN   \n",
      "5                95.0    105.0  ...       135.0    NaN   40.0   NaN   \n",
      "6                96.0     95.0  ...         4.1  102.0    0.9  30.0   \n",
      "7                97.0    100.0  ...       138.0    NaN  102.0   NaN   \n",
      "8                96.0     90.0  ...       137.0    NaN  100.0   NaN   \n",
      "9                98.0     90.0  ...         4.2  100.0    0.8  25.0   \n",
      "10               98.0    100.0  ...       140.0    NaN   98.0   NaN   \n",
      "11               98.0     95.0  ...       140.0    NaN  100.0   NaN   \n",
      "12               98.0     85.0  ...         4.0  100.0    0.8  20.0   \n",
      "13               98.0     95.0  ...       140.0    NaN  100.0   NaN   \n",
      "14               98.0     90.0  ...       140.0    NaN  100.0   NaN   \n",
      "15               95.0    150.0  ...         0.9   30.0   45.0  30.0   \n",
      "16               98.0    180.0  ...       138.0    NaN  100.0   NaN   \n",
      "17                NaN      NaN  ...         NaN    NaN    NaN   NaN   \n",
      "18               96.0    160.0  ...       137.0    NaN   95.0   NaN   \n",
      "19                NaN      NaN  ...         NaN    NaN    NaN   NaN   \n",
      "20               98.0    130.0  ...         4.0  100.0    0.8  25.0   \n",
      "21               98.0    140.0  ...       140.0    NaN  100.0   NaN   \n",
      "22               98.0    120.0  ...       138.0    NaN   98.0   NaN   \n",
      "23               98.0    120.0  ...         4.0  100.0    0.8  20.0   \n",
      "24               98.0    130.0  ...       138.0    NaN  100.0   NaN   \n",
      "25               98.0    110.0  ...       136.0    NaN   98.0   NaN   \n",
      "26               98.0    110.0  ...         4.5   98.0    0.8  18.0   \n",
      "27               98.0    115.0  ...       136.0    NaN  100.0   NaN   \n",
      "28               98.0    100.0  ...       136.0    NaN   95.0   NaN   \n",
      "\n",
      "    Bilirrubina     pH  pCO2   pO2   HCO3  Lactato  \n",
      "0           0.6   7.35  32.0  60.0  16.00      2.5  \n",
      "1           NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "2           NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "3          30.0   0.70   7.4  35.0  85.00     22.0  \n",
      "4           NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "5           NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "6          28.0  27.00   0.5   NaN    NaN      NaN  \n",
      "7           NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "8           NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "9          25.0  25.00   0.5   NaN    NaN      NaN  \n",
      "10          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "11          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "12         25.0  24.00   0.4   NaN    NaN      NaN  \n",
      "13          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "14          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "15          0.8   7.30  28.0  60.0  14.00      4.0  \n",
      "16          NaN    NaN   NaN   NaN   7.35     32.0  \n",
      "17          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "18          NaN    NaN   NaN   NaN   7.35     34.0  \n",
      "19          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "20         80.0  30.00   0.8   NaN    NaN      NaN  \n",
      "21          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "22          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "23         40.0  28.00   0.7   NaN    NaN      NaN  \n",
      "24          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "25          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "26         30.0  28.00   0.7   NaN    NaN      NaN  \n",
      "27          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "28          NaN    NaN   NaN   NaN    NaN      NaN  \n",
      "\n",
      "[29 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "datos_evolucion = pd.read_csv(r\"C:\\Users\\mpord\\Documents\\3IngSoft\\2Cuatri\\G4.Dedalus\\material_dedalus\\resumen_evolucion_process.csv\", sep=\",\", decimal=\".\", on_bad_lines='skip')\n",
    "\n",
    "print(datos_evolucion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Cargar los datos\n",
    "df = datos_evolucion\n",
    "\n",
    "# Convertir columnas a num√©ricas\n",
    "df['LDL'] = pd.to_numeric(df['LDL'], errors='coerce')\n",
    "df['Trigliceridos'] = pd.to_numeric(df['Trigliceridos'], errors='coerce')\n",
    "df['Sodio'] = pd.to_numeric(df['Sodio'], errors='coerce')\n",
    "\n",
    "# Imputar valores faltantes con la mediana (puedes cambiarlo si lo prefieres)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[['PresionSistolica', 'PresionDiastolica', 'FrecuenciaCardiaca', 'Temperatura', 'FrecuenciaRespiratoria', \n",
    "    'SaturacionOxigeno', 'Glucosa', 'Leucocitos', 'Hemoglobina', 'Plaquetas', 'Colesterol', 'HDL', 'LDL', \n",
    "    'Trigliceridos', 'Sodio', 'Potasio', 'Cloro', 'Creatinina', 'Urea', 'AST', 'ALT', 'Bilirrubina', 'pH', \n",
    "    'pCO2', 'pO2', 'HCO3', 'Lactato']] = imputer.fit_transform(df[['PresionSistolica', 'PresionDiastolica', 'FrecuenciaCardiaca', 'Temperatura', \n",
    "                                                                'FrecuenciaRespiratoria', 'SaturacionOxigeno', 'Glucosa', 'Leucocitos', \n",
    "                                                                'Hemoglobina', 'Plaquetas', 'Colesterol', 'HDL', 'LDL', 'Trigliceridos', \n",
    "                                                                'Sodio', 'Potasio', 'Cloro', 'Creatinina', 'Urea', 'AST', 'ALT', \n",
    "                                                                'Bilirrubina', 'pH', 'pCO2', 'pO2', 'HCO3', 'Lactato']])\n",
    "\n",
    "# Normalizar las variables num√©ricas\n",
    "scaler = MinMaxScaler()\n",
    "df[['PresionSistolica', 'PresionDiastolica', 'FrecuenciaCardiaca', 'Temperatura', 'FrecuenciaRespiratoria', \n",
    "    'SaturacionOxigeno', 'Glucosa', 'Leucocitos', 'Hemoglobina', 'Plaquetas', 'Colesterol', 'HDL', 'LDL', \n",
    "    'Trigliceridos', 'Sodio', 'Potasio', 'Cloro', 'Creatinina', 'Urea', 'AST', 'ALT', 'Bilirrubina', 'pH', \n",
    "    'pCO2', 'pO2', 'HCO3', 'Lactato']] = scaler.fit_transform(df[['PresionSistolica', 'PresionDiastolica', 'FrecuenciaCardiaca', 'Temperatura', \n",
    "                                                                'FrecuenciaRespiratoria', 'SaturacionOxigeno', 'Glucosa', 'Leucocitos', \n",
    "                                                                'Hemoglobina', 'Plaquetas', 'Colesterol', 'HDL', 'LDL', 'Trigliceridos', \n",
    "                                                                'Sodio', 'Potasio', 'Cloro', 'Creatinina', 'Urea', 'AST', 'ALT', \n",
    "                                                                'Bilirrubina', 'pH', 'pCO2', 'pO2', 'HCO3', 'Lactato']])\n",
    "\n",
    "# Codificar las variables categ√≥ricas\n",
    "df = pd.get_dummies(df, columns=['PacienteID', 'Fecha', 'Hora'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | D Loss Real: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss_real[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | D Loss Fake: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss_fake[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | G Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Entrenar la GAN\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgan\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 80\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(epochs, batch_size, df, generator, discriminator, gan)\u001b[0m\n\u001b[0;32m     77\u001b[0m labels_fake \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Entrenar el discriminador\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_real\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(generated_data, labels_fake)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Entrenar el generador (la red GAN)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:601\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[1;32m--> 601\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:227\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[0;32m    226\u001b[0m     ):\n\u001b[1;32m--> 227\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:391\u001b[0m, in \u001b[0;36mFunctionType.unpack_inputs\u001b[1;34m(self, bound_parameters)\u001b[0m\n\u001b[0;32m    388\u001b[0m flat \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sorted_parameters:\n\u001b[0;32m    390\u001b[0m   flat\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m--> 391\u001b[0m       \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m   )\n\u001b[0;32m    394\u001b[0m dealiased_inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    395\u001b[0m ids_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\site-packages\\tensorflow\\core\\function\\trace_type\\default_types.py:260\u001b[0m, in \u001b[0;36mTuple.to_tensors\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    258\u001b[0m flattened_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comp_value, comp_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents):\n\u001b[1;32m--> 260\u001b[0m   flattened_values\u001b[38;5;241m.\u001b[39mextend(\u001b[43mcomp_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomp_value\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m flattened_values\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "# Definir el tama√±o de las entradas y el tama√±o del ruido\n",
    "input_dim = df.shape[1]  # n√∫mero de caracter√≠sticas en el conjunto de datos\n",
    "z_dim = 100  # tama√±o del vector de ruido\n",
    "\n",
    "# Construcci√≥n del Generador\n",
    "def build_generator(z_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=z_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(input_dim, activation='tanh'))  # Activaci√≥n 'tanh' para normalizaci√≥n\n",
    "    return model\n",
    "\n",
    "# Construcci√≥n del Discriminador\n",
    "def build_discriminator(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Salida binaria: real o falso\n",
    "    return model\n",
    "\n",
    "# Construcci√≥n del modelo GAN\n",
    "def build_gan(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "# Compilar el Discriminador\n",
    "discriminator = build_discriminator(input_dim)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Compilar el Generador\n",
    "generator = build_generator(z_dim)\n",
    "\n",
    "# El Discriminador debe estar fijo al entrenar el GAN (no se entrena durante el entrenamiento del generador)\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Construir el GAN\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Funci√≥n para entrenar la GAN\n",
    "def train_gan(epochs, batch_size, df, generator, discriminator, gan):\n",
    "    batch_count = df.shape[0] // batch_size\n",
    "\n",
    "    if batch_count == 0:\n",
    "        print(\"Batch size is too large for the dataset.\")\n",
    "        return\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_count):\n",
    "            # Generar datos falsos\n",
    "            noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "            generated_data = generator.predict(noise)\n",
    "\n",
    "            # Obtener una muestra aleatoria de datos reales\n",
    "            real_data = df.sample(batch_size).values\n",
    "\n",
    "            # Etiquetas: 1 para datos reales, 0 para generados\n",
    "            labels_real = np.ones((batch_size, 1))\n",
    "            labels_fake = np.zeros((batch_size, 1))\n",
    "\n",
    "            # Entrenar el discriminador\n",
    "            d_loss_real = discriminator.train_on_batch(real_data, labels_real)\n",
    "            d_loss_fake = discriminator.train_on_batch(generated_data, labels_fake)\n",
    "\n",
    "            # Entrenar el generador (la red GAN)\n",
    "            noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "            g_loss = gan.train_on_batch(noise, labels_real)  # El generador enga√±a al discriminador\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | D Loss Real: {d_loss_real[0]} | D Loss Fake: {d_loss_fake[0]} | G Loss: {g_loss}\")\n",
    "\n",
    "# Entrenar la GAN\n",
    "train_gan(epochs=10000, batch_size=16, df=df, generator=generator, discriminator=discriminator, gan=gan)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
